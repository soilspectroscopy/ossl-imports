---
title: "Dataset import: Garrett et al. (2022)"  
author: "Jose Lucas Safanelli (jsafanelli@woodwellclimate.org) and Jonathan Sanderman (jsanderman@woodwellclimate.org)"  
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  github_document:
    toc: true
    toc_depth: 4
bibliography: ../../tex/refs.bib
csl: ../../tex/apa.csl  
fig_caption: yes
link-citations: yes
twitter-handle: soilspec
header-includes:
- \usepackage{caption}
editor_options: 
  markdown: 
    wrap: 72
---

[<img src="../../img/soilspec4gg-logo_fc.png" alt="SoilSpec4GG logo" width="250"/>](https://soilspectroscopy.org/)

[<img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png"/>](http://creativecommons.org/licenses/by-sa/4.0/)

This work is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).

```{r setup, include=FALSE}
library(captioner)
fig_nums <- captioner(prefix = "Fig.")
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
options(scipen = 999)
```

## The Garrett et al. (2022) Soil Spectral Library

Part of: <https://github.com/soilspectroscopy>  
Project: [Soil Spectroscopy for Global Good](https://soilspectroscopy.org)  
Last update: `r Sys.Date()`  
Dataset: [GARRETT.SSL](https://soilspectroscopy.github.io/ossl-manual/soil-spectroscopy-tools-and-users.html#garrett.ssl)

Mid-Infrared Spectra (MIRS) of 186 soil samples from forest soils of New Zealand (Surveyed by Scion Research) described in @Garrett2022.

The dataset is publicly shared at Figshare <https://doi.org/10.6084/m9.figshare.20506587.v2>.

Input datasets:  
- `FR380_sitedescription.xlsx` and `FR380_soilprofile.xlsx`: files with site information;  
- `FR380_physical.xlsx`, `FR380_chemical.xlsx`, and `FR380_particlesize.xlsx`: files with soil information;  
- `FR380_MIR spectra_csv`: folder with exported MIR spectral scans;

```{r packages, include=TRUE, echo=FALSE, eval=TRUE}
packages <- c("tidyverse", "prospectr", "measurements", "readxl", "stringr",
              "olctools", "openssl", "tmap", "sf", "skimr", "lubridate",
              "googledrive", "googlesheets4", "data.table", "doMC", "tictoc", "qs")
new.packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
invisible(lapply(packages, library, character.only = TRUE))
source("../../R-code/functions/SSL_functions.R")
```

Directory/folder path:
```{r}
dir = "/mnt/soilspec4gg/ossl/dataset/Garrett/"
tic()
```

## Data import

### Soil site information

Spectral data filenames follow Scion_Sample ID present in chemical data, but there are other id columns from LCR and site ids that are necessary for binding with other tables (like physical). In this case, Scion_Sample ID will be used as `id.layer_local_c` in the OSSL.

```{r}
# Getting FR380_chemical
garrett.chemical <- readxl::read_xlsx(paste0(dir, "/FR380_chemical.xlsx"), sheet = "FR380_Chemical", skip = 1)

garrett.ids <- garrett.chemical %>%
  select(`Scion_Sample ID`, `Trial ID`,
         `LCR_Sample ID`, `LCR_Soil profile ID`,
         `LCR_Lab letter`, `LCR_Horizon number`,
         `Horizon top (cm)`, `Horizon base (cm)`,) %>%
  rename(id.layer_local_c = `Scion_Sample ID`,
         id.dataset.site_ascii_c = `Trial ID`,
         layer.upper.depth_usda_cm = `Horizon top (cm)`,
         layer.lower.depth_usda_cm = `Horizon base (cm)`) %>%
  mutate(id.layer_local_c = as.character(id.layer_local_c)) %>%
  filter(!is.na(id.layer_local_c)) %>%
  mutate(id.dataset.site_ascii_c = gsub("\\s", "", id.dataset.site_ascii_c))

# Getting FR380_sitedescription
garrett.sitedescription <- readxl::read_xlsx(paste0(dir, "/FR380_sitedescription.xlsx"), sheet = "FR380_site description")

# Getting FR380_soilprofile
garrett.soilprofile <- readxl::read_xlsx(paste0(dir, "/FR380_soilprofile.xlsx"), sheet = "FR380_soil profile")

# Preparing soilsite information
garrett.sitedata <- garrett.sitedescription %>%
  select(`Trial ID`, `Date observed`, `Latitude (째)`, `Longitude (째)`, `Soil type`) %>%
  rename(longitude.point_wgs84_dd = `Longitude (째)`, latitude.point_wgs84_dd = `Latitude (째)`,
         id.dataset.site_ascii_c = `Trial ID`, layer.texture_usda_txt = `Soil type`) %>%
  mutate(id.dataset.site_ascii_c = gsub("\\s", "", id.dataset.site_ascii_c)) %>%
  mutate(`Date observed` = lubridate::ymd(`Date observed`)) %>%
  mutate(observation.date.begin_iso.8601_yyyy.mm.dd = stringr::str_c(lubridate::year(`Date observed`),
                                                                     lubridate::month(`Date observed`),
                                                                     lubridate::day(`Date observed`),
                                                                     sep = "."),
         observation.date.end_iso.8601_yyyy.mm.dd = stringr::str_c(lubridate::year(`Date observed`),
                                                                   lubridate::month(`Date observed`),
                                                                   lubridate::day(`Date observed`),
                                                                   sep = ".")) %>%
  select(id.dataset.site_ascii_c,
         longitude.point_wgs84_dd, latitude.point_wgs84_dd,
         observation.date.begin_iso.8601_yyyy.mm.dd,
         observation.date.end_iso.8601_yyyy.mm.dd,
         layer.texture_usda_txt) %>%
  left_join({garrett.ids %>%
      select(-contains("LCR"))}, ., by = "id.dataset.site_ascii_c") %>%
  mutate(id.layer_uuid_txt = openssl::md5(id.layer_local_c), # Adding missing metadata
         id.location_olc_txt = olctools::encode_olc(latitude.point_wgs84_dd, longitude.point_wgs84_dd, 10),
         id.project_ascii_txt = "Forest soil data from New Zealand (Scion Research)",
         observation.ogc.schema.title_ogc_txt = 'Open Soil Spectroscopy Library',
         observation.ogc.schema_idn_url = 'https://soilspectroscopy.github.io',
         pedon.taxa_usda_txt = "",
         horizon.designation_usda_txt = "",
         longitude.county_wgs84_dd = NA,
         latitude.county_wgs84_dd = NA,
         location.country_iso.3166_txt = "NZL",
         location.point.error_any_m = 1111, # Only two decimal places in lat long
         surveyor.title_utf8_txt = "Loretta Garrett",
         surveyor.contact_ietf_email = "loretta.garrett@scionresearch.com",
         surveyor.address_utf8_txt = 'Scion, Private Bag 3020, Rotorua 3046, New Zealand',
         dataset.title_utf8_txt = 'Garrett et al. (2022)',
         dataset.owner_utf8_txt = 'Garrett et al. (2022)',
         dataset.code_ascii_txt = 'GARRETT.SSL',
         dataset.address_idn_url = 'https://doi.org/10.6084/m9.figshare.20506587.v2',
         dataset.license.title_ascii_txt = 'CC-BY',
         dataset.license.address_idn_url = 'https://creativecommons.org/licenses/by/4.0/legalcode',
         dataset.doi_idf_url = 'https://doi.org/10.6084/m9.figshare.20506587.v2',
         dataset.contact.name_utf8_txt = "Loretta Garrett",
         dataset.contact_ietf_email = "loretta.garrett@scionresearch.com") %>%
  select(id.layer_uuid_txt, # Following the sequence from ossl-manual
         id.layer_local_c,
         id.location_olc_txt,
         id.dataset.site_ascii_c,
         id.project_ascii_txt,
         observation.date.begin_iso.8601_yyyy.mm.dd,
         observation.date.end_iso.8601_yyyy.mm.dd,
         longitude.point_wgs84_dd,
         latitude.point_wgs84_dd,
         pedon.taxa_usda_txt,
         layer.texture_usda_txt,
         horizon.designation_usda_txt,
         longitude.county_wgs84_dd,
         latitude.county_wgs84_dd,
         location.country_iso.3166_txt,
         location.point.error_any_m,
         observation.ogc.schema.title_ogc_txt,
         observation.ogc.schema_idn_url,
         surveyor.title_utf8_txt,
         surveyor.contact_ietf_email,
         surveyor.address_utf8_txt,
         dataset.title_utf8_txt,
         dataset.owner_utf8_txt,
         dataset.code_ascii_txt,
         dataset.address_idn_url,
         dataset.license.title_ascii_txt,
         dataset.license.address_idn_url,
         dataset.doi_idf_url,
         dataset.contact.name_utf8_txt,
         dataset.contact_ietf_email) %>%
  mutate_at(vars(starts_with("id.")), as.character)

# Saving version to dataset root dir
site.qs = paste0(dir, "/ossl_soilsite_v1.2.qs")
qs::qsave(garrett.sitedata, site.qs, preset = "high")
```

### Soil lab information

NOTE: The code chunk below this paragraph is hidden. Just run once for getting the original names of soil properties, descriptions, data types, and units. Run once and upload to Google Sheet for formatting and integrating with the OSSL. Requires Google authentication.

<!-- ```{r, eval=FALSE, echo=TRUE} -->
<!-- garrett.physical.desc <- readxl::read_xlsx(paste0(dir, "/FR380_physical.xlsx"), sheet = "Data dictionary") -->
<!-- garrett.chemical.desc <- readxl::read_xlsx(paste0(dir, "/FR380_chemical.xlsx"), sheet = "Data dictionary") -->
<!-- garrett.particlesize.desc <- readxl::read_xlsx(paste0(dir, "/FR380_particlesize.xlsx"), sheet = "Data dictionary") -->

<!-- soillab.names <- garrett.particlesize.desc %>% -->
<!--   dplyr::mutate(table = "FR380_particlesize", .before = 1) %>% -->
<!--   dplyr::rename(original_name = `Field name`, original_description = `Field name description`, -->
<!--          comment1 = `Specific test method`, comment2 = `Relevant reference`) %>% -->
<!--   mutate(comment = paste0(comment1, "; ", comment2)) %>% -->
<!--   dplyr::select(table, original_name, original_description, comment) %>% -->
<!--   bind_rows({ -->
<!--     garrett.physical.desc %>% -->
<!--       dplyr::mutate(table = "FR380_physical", .before = 1) %>% -->
<!--       dplyr::rename(original_name = `Field name`, original_description = `Field name description`, -->
<!--                     comment1 = `Specific test method`, comment2 = `Relevant reference`) %>% -->
<!--   mutate(comment = paste0(comment1, "; ", comment2)) %>% -->
<!--       dplyr::select(table, original_name, original_description, comment) -->
<!--   }) %>% -->
<!--   bind_rows({ -->
<!--     garrett.chemical.desc %>% -->
<!--       dplyr::mutate(table = "FR380_chemical", .before = 1) %>% -->
<!--       dplyr::rename(original_name = `Field name`, original_description = `Field name description`, -->
<!--                     comment1 = `Specific chemical test method`, comment2 = `Relevant reference`) %>% -->
<!--       mutate(comment = paste0(comment1, "; ", comment2)) %>% -->
<!--       dplyr::select(table, original_name, original_description, comment) -->
<!--   }) %>% -->
<!--   dplyr::mutate(import = '', ossl_name = '', .after = original_name) -->

<!-- readr::write_csv(soillab.names, paste0(getwd(), "/garrett_soillab_names.csv")) -->

<!-- # Uploading to google sheet -->

<!-- # FACT CIN folder. Get ID for soildata importing table -->
<!-- googledrive::drive_ls(as_id("0AHDIWmLAj40_Uk9PVA")) -->

<!-- OSSL.soildata.importing <- "19LeILz9AEnKVK7GK0ZbK3CCr2RfeP-gSWn5VpY8ETVM" -->

<!-- # Checking metadata -->
<!-- googlesheets4::as_sheets_id(OSSL.soildata.importing) -->

<!-- # Checking readme -->
<!-- googlesheets4::read_sheet(OSSL.soildata.importing, sheet = 'readme') -->

<!-- # Preparing soillab.names -->
<!-- upload <- dplyr::as_tibble(soillab.names) -->

<!-- # Uploading -->
<!-- googlesheets4::write_sheet(upload, ss = OSSL.soildata.importing, sheet = "Garrett") -->

<!-- # Checking metadata -->
<!-- googlesheets4::as_sheets_id(OSSL.soildata.importing) -->
<!-- ``` -->

NOTE: The code chunk below this paragraph is hidden. Run once for importing the transformation rules. The table can be edited online at Google Sheets. A copy is downloaded to github for archiving.

<!-- ```{r soilab_download, include=FALSE, echo=FALSE, eval=FALSE} -->
<!-- # Downloading from google sheet -->

<!-- # FACT CIN folder id -->
<!-- listed.table <- googledrive::drive_ls(as_id("0AHDIWmLAj40_Uk9PVA"), -->
<!--                                       pattern = "OSSL_tab2_soildata_importing") -->

<!-- OSSL.soildata.importing <- listed.table[[1,"id"]] -->

<!-- # Checking metadata -->
<!-- googlesheets4::as_sheets_id(OSSL.soildata.importing) -->

<!-- # Preparing soillab.names -->
<!-- transvalues <- googlesheets4::read_sheet(OSSL.soildata.importing, sheet = "Garrett") %>% -->
<!--   filter(import == TRUE) %>% -->
<!--   select(contains(c("table", "id", "original_name", "ossl_"))) -->

<!-- # Saving to folder -->
<!-- write_csv(transvalues, paste0(getwd(), "/OSSL_transvalues.csv")) -->
<!-- ``` -->

Reading AFSIS1-to-OSSL transformation values:
```{r soilab_transvalues, include=TRUE, echo=TRUE, eval=TRUE}
transvalues <- read_csv(paste0(getwd(), "/OSSL_transvalues.csv"))
knitr::kable(transvalues)
```

```{r soilab_preparation, include=TRUE, echo=TRUE, eval=TRUE}
# Getting FR380_chemical
garrett.chemical <- readxl::read_xlsx(paste0(dir, "/FR380_chemical.xlsx"), sheet = "FR380_Chemical", skip = 1)

# Getting FR380_physical
garrett.physical <- readxl::read_xlsx(paste0(dir, "/FR380_physical.xlsx"), sheet = "FR380_Physical")

# Getting FR380_particlesize
garrett.particlesize <- readxl::read_xlsx(paste0(dir, "/FR380_particlesize.xlsx"), sheet = "FR380_Particle size", skip = 0)

# Harmonization of names and units for chemical data
analytes.old.names <- transvalues %>%
  filter(table == "FR380_chemical") %>%
  pull(original_name)

analytes.new.names <- transvalues %>%
  filter(table == "FR380_chemical") %>%
  pull(ossl_name)

garrett.soil.chemical <- garrett.chemical %>%
  rename(id.layer_local_c = `Scion_Sample ID`,
         layer.upper.depth_usda_cm = `Horizon top (cm)`,
         layer.lower.depth_usda_cm = `Horizon base (cm)`) %>%
  mutate(id.layer_local_c = as.character(id.layer_local_c)) %>%
  select(id.layer_local_c, layer.upper.depth_usda_cm, layer.lower.depth_usda_cm, all_of(analytes.old.names)) %>%
  rename_with(~analytes.new.names, analytes.old.names) %>%
  mutate_at(vars(-id.layer_local_c), as.numeric) %>%
  as.data.frame()

# Harmonization of names and units for psd data
analytes.old.names <- transvalues %>%
  filter(table == "FR380_particlesize") %>%
  pull(original_name)

analytes.new.names <- transvalues %>%
  filter(table == "FR380_particlesize") %>%
  pull(ossl_name)

garrett.soil.psd <- garrett.particlesize %>%
  rename_with(~analytes.new.names, analytes.old.names) %>%
  select(`LCR_Soil profile ID`, `LCR_Lab letter`, all_of(analytes.new.names)) %>%
  left_join(garrett.ids, by = c("LCR_Soil profile ID", "LCR_Lab letter")) %>%
  select(id.layer_local_c, all_of(analytes.new.names))

# Harmonization of names and units for physical data
analytes.old.names <- transvalues %>%
  filter(table == "FR380_physical") %>%
  pull(original_name)

analytes.new.names <- transvalues %>%
  filter(table == "FR380_physical") %>%
  pull(ossl_name)

garrett.soil.physical <- garrett.physical %>%
  filter(`Sample plots 'Disturbed' or 'Undisturbed'` == "Undisturbed") %>%
  rename_with(~analytes.new.names, analytes.old.names) %>%
  select(`LCR_Soil profile ID`, `LCR_Horizon number`, all_of(analytes.new.names)) %>%
  left_join(garrett.ids, by = c("LCR_Soil profile ID", "LCR_Horizon number")) %>%
  select(id.layer_local_c, all_of(analytes.new.names)) %>%
  mutate_at(vars(-c('id.layer_local_c')), as.numeric) %>%
  filter(!is.na(id.layer_local_c))
  
analytes.new.names <- transvalues %>%
  pull(ossl_name)

garrett.soildata <- garrett.ids %>%
  select(id.layer_local_c) %>%
  left_join(garrett.soil.psd, by = "id.layer_local_c") %>%
  left_join(garrett.soil.chemical, by = "id.layer_local_c") %>%
  left_join(garrett.soil.physical, by = "id.layer_local_c") %>%
  group_by(id.layer_local_c) %>%
  summarise_all(first, .group = "drop") %>%
  select(id.layer_local_c, layer.upper.depth_usda_cm, layer.lower.depth_usda_cm, all_of(analytes.new.names)) %>%
  as.data.frame()

# Getting the formulas
functions.list <- transvalues %>%
  mutate(ossl_name = factor(ossl_name, levels = names(garrett.soildata))) %>%
  arrange(ossl_name) %>%
  pull(ossl_convert) %>%
  c("x", "x", "x", .)

# Applying transformation rules
garrett.soildata.trans <- transform_values(df = garrett.soildata,
                                           out.name = names(garrett.soildata),
                                           in.name = names(garrett.soildata),
                                           fun.lst = functions.list)

# Final soillab data
garrett.soildata <- garrett.soildata.trans %>%
  mutate_at(vars(starts_with("id.")), as.character)

# Checking total number of observations
garrett.soildata %>%
  distinct(id.layer_local_c) %>%
  summarise(count = n())
  
# Saving version to dataset root dir
soillab.qs = paste0(dir, "/ossl_soillab_v1.2.qs")
qs::qsave(garrett.soildata, soillab.qs, preset = "high")
```

### Mid-infrared spectroscopy data

```{r mir, include=TRUE, echo=TRUE, eval=TRUE}
# Spectral measurements
scans.csv <- list.files(paste0(dir, "/FR380_MIR spectra_csv"), full.names = TRUE)
scans.names <- list.files(paste0(dir, "/FR380_MIR spectra_csv"), full.names = FALSE)

# Spectra is stored in long format without header, first column wavenumber, second column absorbance
mir.test <- readr::read_csv(scans.csv[1], show_col_types = FALSE, col_names = FALSE) %>%
  setNames(c("wavenumber", "absorbance"))

mir.allspectra <- purrr::map_dfr(.x = scans.csv, .f = fread, .id = "source", header = FALSE)

mir.allspectra <- mir.allspectra %>%
  pivot_wider(names_from = "V1", values_from = "V2") %>%
  mutate(id = scans.names, .before = 1) %>%
  mutate(id = gsub(".csv", "", id))

# Checking number of spectral replicates
mir.allspectra %>%
  separate(id, into = c("id", "replicate"), sep = "-") %>%
  group_by(id) %>%
  summarise(n = n()) %>%
  group_by(n) %>%
  summarise(count = n())

# Checking number of unique spectral samples
mir.allspectra %>%
  separate(id, into = c("id", "replicate"), sep = "-") %>%
  group_by(id) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  nrow()

# Removing source column (it comes from csv importing when mapping listed files. number is row/object id)
garrett.mir <- mir.allspectra %>%
  select(-source) %>%
  rename(scan.file_any_c = id) %>%
  mutate(id.scan_local_c = scan.file_any_c, .after = scan.file_any_c) %>%
  separate(id.scan_local_c, into = c("id.scan_local_c", "table_code"), sep = "_") %>%
  select(-table_code) %>%
  mutate(id.layer_local_c = str_sub(id.scan_local_c, 1, -3), .before = 1) %>%
  mutate(id.layer_local_c = as.character(id.layer_local_c))

# The spectra is already formatted between 600-4000 cm-1
# But it is necessary to average them
old.wavenumbers <- seq(600, 4000, by = 2)
new.wavenumbers <- paste0("scan_mir.", old.wavenumbers, "_abs")

garrett.mir <- garrett.mir %>%
  select(id.layer_local_c, as.character(rev(old.wavenumbers))) %>%
  rename_with(~new.wavenumbers, as.character(old.wavenumbers)) %>%
  group_by(id.layer_local_c) %>%
  summarize_all(mean)
  
# Preparing metadata
garrett.mir.metadata <- garrett.mir %>%
  select(id.layer_local_c) %>%
  mutate(id.scan_local_c = id.layer_local_c) %>%
  mutate(scan.mir.date.begin_iso.8601_yyyy.mm.dd = ymd("2019-07-01"),
         scan.mir.date.end_iso.8601_yyyy.mm.dd = ymd("2019-08-31"),
         scan.mir.model.name_utf8_txt = "Bruker Invenio-S with HTS-XT",
         scan.mir.model.code_any_txt = "Bruker_InvenioS_HTS.XT",
         scan.mir.method.light.source_any_txt = "",
         scan.mir.method.preparation_any_txt = "",
         scan.mir.license.title_ascii_txt = "CC-BY",
         scan.mir.license.address_idn_url = "https://creativecommons.org/licenses/by/4.0/",
         scan.mir.doi_idf_url = "https://doi.org/10.6084/m9.figshare.20506587.v2",
         scan.mir.contact.name_utf8_txt = "Loretta Garrett",
         scan.mir.contact.email_ietf_txt = "loretta.garrett@scionresearch.com")

# Final preparation
garrett.mir.export <- garrett.mir.metadata %>%
  left_join(garrett.mir, by = "id.layer_local_c") %>%
  mutate_at(vars(starts_with("id.")), as.character)

# Saving version to dataset root dir
soilmir.qs = paste0(dir, "/ossl_mir_v1.2.qs")
qs::qsave(garrett.mir.export, soilmir.qs, preset = "high")
```

### Quality control

The final table must be joined as:

- MIR is used as first reference.
- Then it is left joined with the site and soil lab data. This drop data without any scan.

The availabilty of data is summarised below:
```{r bind_test, include=TRUE, echo=TRUE, eval=TRUE}
# Taking a few representative columns for checking the consistency of joins
garrett.availability <- garrett.mir.export %>%
  select(id.layer_local_c, scan_mir.600_abs) %>%
  left_join({garrett.sitedata %>%
      select(id.layer_local_c, latitude.point_wgs84_dd)}, by = "id.layer_local_c") %>%
  left_join({garrett.soildata %>%
      select(id.layer_local_c, c.tot_usda.a622_w.pct)}, by = "id.layer_local_c") %>%
  filter(!is.na(id.layer_local_c))

# Availability of information from garrett
garrett.availability %>%
  mutate_all(as.character) %>%
  pivot_longer(everything(), names_to = "column", values_to = "value") %>%
  filter(!is.na(value)) %>%
  group_by(column) %>%
  summarise(count = n())

# Repeats check - Duplicates are dropped
garrett.availability %>%
  mutate_all(as.character) %>%
  select(id.layer_local_c) %>%
  pivot_longer(everything(), names_to = "column", values_to = "value") %>%
  group_by(column, value) %>%
  summarise(repeats = n()) %>%
  group_by(column, repeats) %>%
  summarise(count = n())
```

Plotting sites map:
```{r map, include=TRUE, echo=TRUE, eval=TRUE}
data("World")

points <- garrett.sitedata %>%
  filter(!is.na(longitude.point_wgs84_dd)) %>%
  st_as_sf(coords = c('longitude.point_wgs84_dd', 'latitude.point_wgs84_dd'), crs = 4326)

tmap_mode("plot")

tm_shape(World) +
  tm_polygons('#f0f0f0f0', border.alpha = 0.2) +
  tm_shape(points) +
  tm_dots()
```

Soil analytical data summary. Note: many scans could not be linked with the wetchem.
```{r summary, include=TRUE, echo=TRUE, eval=TRUE}
garrett.soildata %>%
  mutate(id.layer_local_c = factor(id.layer_local_c)) %>%
  skimr::skim() %>%
  dplyr::select(-numeric.hist, -complete_rate)
```

MIR spectral visualization:
```{r mir_plot, include=TRUE, echo=TRUE, eval=TRUE}
garrett.mir %>%
  select(all_of(c("id.layer_local_c")), starts_with("scan_mir.")) %>%
  tidyr::pivot_longer(-all_of(c("id.layer_local_c")),
                      names_to = "wavenumber", values_to = "absorbance") %>%
  dplyr::mutate(wavenumber = gsub("scan_mir.|_abs", "", wavenumber)) %>%
  dplyr::mutate(wavenumber = as.numeric(wavenumber)) %>%
  ggplot(aes(x = wavenumber, y = absorbance, group = id.layer_local_c)) +
  geom_line(alpha = 0.1) +
  scale_x_continuous(breaks = c(600, 1200, 1800, 2400, 3000, 3600, 4000)) +
  labs(x = bquote("Wavenumber"~(cm^-1)), y = "Absorbance") +
  theme_light()
```

```{r}
toc()
rm(list = ls())
gc()
```

## References
