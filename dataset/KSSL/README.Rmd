---
title: "Dataset import: Kellogg Soil Survey Laboratory (KSSL)"
author: "Jose Lucas Safanelli (jsafanelli@woodwellclimate.org), Tomislav Hengl (tom.hengl@opengeohub.org), Jonathan Sanderman (jsanderman@woodwellclimate.org), Develyn Bloom (develyn.bloom@ufl.edu) - "
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  github_document:
    toc: true
    toc_depth: 4
bibliography: ../../tex/refs.bib
csl: ../../tex/apa.csl  
fig_caption: yes
link-citations: yes
twitter-handle: soilspec
header-includes:
- \usepackage{caption}
---

[<img src="../../img/soilspec4gg-logo_fc.png" alt="SoilSpec4GG logo" width="250"/>](https://soilspectroscopy.org/)

[<img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" />](http://creativecommons.org/licenses/by-sa/4.0/)

This work is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).

```{r setup, include=FALSE}
library(captioner)
fig_nums <- captioner(prefix = "Fig.")
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
options(scipen = 999)
```

## Kellogg Soil Survey Laboratory inputs

Part of: <https://github.com/soilspectroscopy>  
Project: [Soil Spectroscopy for Global
Good](https://soilspectroscopy.org)  
Last update: `r Sys.Date()`  
Dataset: [KSSL.SSL](https://soilspectroscopy.github.io/ossl-manual/soil-spectroscopy-tools-and-users.html#kssl.ssl)

The USDA-NRCS NCSS Kellogg Soil Survey Laboratory has a large and growing 
mid-infrared (MIR) spectral library. Calibration models are being developed 
to predict soil properties from MIR spectra. Dataset properties are explained 
in detail in @wijewardane2018predicting and @sanderman2020mid.

Input datasets (snapshot from Jul 2022):

- `KSSL_202207_MIR_spectra_all_avg.csv`: csv/rds file with averaged MIR scans;
- `RaCA_measured.csv`: csv/rds file with averaged VNIR scans (from [RaCA project](https://www.nrcs.usda.gov/resources/data-and-reports/rapid-carbon-assessment-raca));
- `All_Spectra_Access_Portable_20220712`: a folder with all tables exported from a Microsoft Access database;

For the DB structure and use refer to "Introduction to the KSSL Laboratory Information Management System" contacts: Rich Ferguson & Scarlett Murphy (NRCS USDA).

```{r packages, include=TRUE, echo=FALSE, eval=TRUE}
packages <- c("tidyverse", "prospectr", "measurements", "readxl", "stringr",
              "olctools", "openssl", "tmap", "sf", "skimr", "lubridate",
              "googledrive", "googlesheets4", "data.table", "doMC", "tictoc", "qs")
new.packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
invisible(lapply(packages, library, character.only = TRUE))
source("../../R-code/functions/SSL_functions.R")
```

The directory/folder path:

```{r dir}
dir.files = "/mnt/soilspec4gg/ossl/dataset/KSSL/snapshot_Jul2022"
tic()
```

<!-- Load customized functions: -->
<!-- ```{r, echo=FALSE, eval=FALSE} -->
<!-- #load.pigz(paste0(dir, "KSSL.RData")) -->
<!-- source("../../R-code/functions/SSL_functions.R") -->
<!-- ``` -->

<!-- ## Data import -->

### Soil site information

```{r soilsite, include=TRUE, echo=TRUE, eval=TRUE}
dir.db <- paste0(dir.files, "/All_Spectra_Access_Portable_20220712")

# Reading tables with site info
layer <- fread(paste0(dir.db, "/layer.csv"))
sample <- fread(paste0(dir.db, "/sample.csv"))
project <- fread(paste0(dir.db, "/project.csv"))
lims.site <- fread(paste0(dir.db, "/lims_site.csv"))
lims.pedtax <- fread(paste0(dir.db, "/lims_ped_tax_hist.csv"))
centroid <- fread(paste0(dir.db, "/centroid.csv"))
area <- fread(paste0(dir.db, "/area.csv"))
site.area <- fread(paste0(dir.db, "/site_area_overlap.csv"))

# Layer table contains only lay.id and horizon info
layer <- layer %>%
  select(lay.id, lay.type, proj.id, lims.site.id, lims.pedon.id,
         lay.depth.to.top, lay.depth.to.bottom, horizon.designation, texture.description)

# Sample table has lay and smp ids for linking all data in the database
sample <- sample %>%
  select(smp.id, lay.id)

# Project table with date of observation and project name
project <- project %>%
  select(proj.id, fiscal.year, submit.proj.name)

# Coordinates
lims.site <- lims.site %>%
  select(lims.site.id, horizontal.datum.name,
         latitude.std.decimal.degrees, longitude.std.decimal.degrees)

# Taxonomic classification
lims.pedtax <- lims.pedtax %>%
  select(lims.pedon.id, taxonomic.classification.name) %>%
  group_by(lims.pedon.id) %>%
  summarise_all(first)

# County centroids for additional spatial reference
centroid <- centroid %>%
  select(area.id, lat.ycntr, long.xcntr, fips.code)

# Area info
area <- area %>%
  select(area.id, area.type, area.name, area.code)

# Link between layers
site.area <- site.area %>%
  select(lims.site.id, area.id)

# Site area with county coordiantes
site.overview <- site.area %>%
  left_join(area, by = "area.id") %>%
  left_join(centroid, by = "area.id") %>%
  filter(area.type %in% c("county", "country")) %>%
  pivot_wider(names_from = "area.type",
              values_from = c("area.id", "area.name", "area.code")) %>%
  group_by(lims.site.id) %>%
  summarise_all(~first(na.omit(.))) %>%
  left_join(lims.site, by = "lims.site.id")
  
# Joining all data
kssl.sitedata <- layer %>%
  left_join(sample, by = "lay.id") %>%
  relocate(smp.id, .after = "lay.id") %>%
  left_join(project, by = "proj.id") %>%
  left_join(lims.pedtax, by = "lims.pedon.id") %>%
  left_join(site.overview, by = "lims.site.id") %>%
  as_tibble()

# Cleaning repeats and internal projects
kssl.sitedata <- kssl.sitedata %>%
  filter(!(proj.id == 1)) # This proj is used internally

# Just a few repeats, so we can first them
# repeat.ids <- kssl.sitedata %>%
#   group_by(lay.id) %>%
#   summarise(repeats = n()) %>%
#   filter(repeats > 1) %>%
#   pull(lay.id)
# 
# kssl.sitedata %>% filter(lay.id %in% repeat.ids)

kssl.sitedata <- kssl.sitedata %>%
  group_by(lay.id) %>%
  summarise_all(first)

# Available datums
datum.summary <- kssl.sitedata %>%
  rename(hor.datum = horizontal.datum.name) %>%
  group_by(hor.datum) %>%
  summarise(count = n())

datum.summary

# We will assume layers with NA, empty and NULL in the datum column as NAD83
# The error between this US system is way smaller than assuming a county centroid
kssl.sitedata <- kssl.sitedata %>%
  mutate(horizontal.datum.name = ifelse(is.na(latitude.std.decimal.degrees),
                            NA, 
                            ifelse(horizontal.datum.name == '' |
                                     horizontal.datum.name == 'NULL' |
                                     is.na(horizontal.datum.name),
                                   "NAD83",
                                   horizontal.datum.name)))

# Corrected datums
datum.summary.cor <- kssl.sitedata %>%
  rename(hor.datum = horizontal.datum.name) %>%
  group_by(hor.datum) %>%
  summarise(count = n())

datum.summary.cor

# Transforming all datums to WGS84
datums <- kssl.sitedata %>%
  distinct(horizontal.datum.name) %>%
  pull(horizontal.datum.name)

projection.list <- list()

for(i in 1:length(datums)) {
  
  idatum <- datums[i]
  
  if(is.na(idatum)) {
    
    newdata <- kssl.sitedata %>%
      filter(is.na(horizontal.datum.name))
    
  } else if(idatum == "NAD27") {
    
    idata <- kssl.sitedata %>%
      filter(horizontal.datum.name == idatum)
    
    new.coords <- st_as_sf(idata,
                        crs = 4267,
                        coords = c("longitude.std.decimal.degrees",
                                   "latitude.std.decimal.degrees")) %>%
      st_transform(crs = 4326)
    
    newdata <- idata %>%
      mutate(longitude.std.decimal.degrees = st_coordinates(new.coords)[,1],
             latitude.std.decimal.degrees = st_coordinates(new.coords)[,2],
             horizontal.datum.name = "WGS84")
    
  }  else if(idatum == "NAD83") {
    
    idata <- kssl.sitedata %>%
      filter(horizontal.datum.name == idatum)
    
    new.coords <- st_as_sf(idata,
                        crs = 4269,
                        coords = c("longitude.std.decimal.degrees",
                                   "latitude.std.decimal.degrees")) %>%
      st_transform(crs = 4326) 
    
    newdata <- idata %>%
      mutate(longitude.std.decimal.degrees = st_coordinates(new.coords)[,1],
             latitude.std.decimal.degrees = st_coordinates(new.coords)[,2],
             horizontal.datum.name = "WGS84")
    
  } else if(idatum == "old hawaiian") {
    
    idata <- kssl.sitedata %>%
      filter(horizontal.datum.name == idatum)
    
    new.coords <- st_as_sf(idata,
                        crs = 4135,
                        coords = c("longitude.std.decimal.degrees",
                                   "latitude.std.decimal.degrees")) %>%
      st_transform(crs = 4326) 
    
    newdata <- idata %>%
      mutate(longitude.std.decimal.degrees = st_coordinates(new.coords)[,1],
             latitude.std.decimal.degrees = st_coordinates(new.coords)[,2],
             horizontal.datum.name = "WGS84")
    
  } else {
    
    newdata <- kssl.sitedata %>%
      filter(horizontal.datum.name == idatum)
    
  }
  
  projection.list[[i]] <- newdata
  
}

kssl.sitedata <- Reduce(bind_rows, projection.list)

# Correcting incorrect coordinates
check.data <- kssl.sitedata %>%
  filter(longitude.std.decimal.degrees > 0 |
           latitude.std.decimal.degrees < 10) %>%
  select(contains(c("id", "area", "proj", "degrees")))

# Getting project names and ids with issues for comparing with good data
projects <- check.data %>%
  distinct(submit.proj.name) %>%
  pull(submit.proj.name)

wrong.ids <- check.data %>%
  distinct(lay.id) %>%
  pull(lay.id)

# Manually/Visually checking
wrong.data <- kssl.sitedata %>%
  select(contains(c("id", "area", "proj", "degrees"))) %>%
  filter(submit.proj.name %in% projects) %>%
  mutate(wrong = ifelse(lay.id %in% wrong.ids, TRUE, FALSE), .after = 1)

# Unrecoverable coordinates, mistyped?
id.pedons.na <- c(10134, 11930, 12450, 12451, 12452, 12453)

# Pedon ids that need a negative long
id.pedons.neg.longitude <- c(12776, 12778, 12779, 32888, 34509,
                             35124, 35125, 36777, 38043, 38958,
                             39334, 40135)

# Pedon ids that need reverse lat/long, and negative long
id.pedons.reverse.neg.longitude <- c(39332, 39333)

# Site ids that need a negative long
id.site.neg.longitude <- c(33668)

# Final corrected version
kssl.sitedata <- kssl.sitedata %>%
  mutate(longitude.std.decimal.degrees = ifelse(lims.pedon.id %in% id.pedons.na,
                                                NA,
                                                longitude.std.decimal.degrees),
         latitude.std.decimal.degrees = ifelse(lims.pedon.id %in% id.pedons.na, 
                                               NA, 
                                               latitude.std.decimal.degrees)) %>%
  mutate(longitude.std.decimal.degrees = ifelse(lims.pedon.id %in% id.pedons.neg.longitude, 
                                                longitude.std.decimal.degrees*-1, 
                                                longitude.std.decimal.degrees)) %>%
  mutate(longitude.std.decimal.degrees = ifelse(lims.site.id %in% id.site.neg.longitude, 
                                                longitude.std.decimal.degrees*-1, 
                                                longitude.std.decimal.degrees)) %>%
  mutate(temp.long = longitude.std.decimal.degrees,
         temp.lat = latitude.std.decimal.degrees,
         longitude.std.decimal.degrees = ifelse(lims.pedon.id %in% id.pedons.reverse.neg.longitude, 
                                                latitude.std.decimal.degrees*-1, 
                                                longitude.std.decimal.degrees),
         latitude.std.decimal.degrees = ifelse(lims.pedon.id %in% id.pedons.reverse.neg.longitude, 
                                               temp.long, 
                                               temp.lat)) %>%
  select(-temp.long, -temp.lat)

# Summary of pedons, layers and coordinates
sitedata.summary <- kssl.sitedata %>%
      summarise(var = "total_layers", count = n()) %>%
  bind_rows(., tibble(var = "total_profiles", count = {kssl.sitedata %>%
      group_by(lims.pedon.id) %>%
      summarise(first_horizon = first(lay.id)) %>%
      ungroup() %>%
      summarise(count = n()) %>%
      pull(count)})) %>%
  bind_rows(., tibble(var = "profiles_with_point_coords", count = {kssl.sitedata %>%
      group_by(lims.pedon.id) %>%
      summarise(latitude.std.decimal.degrees = mean(latitude.std.decimal.degrees)) %>%
      filter(!is.na(latitude.std.decimal.degrees)) %>%
      ungroup() %>%
      summarise(count = n()) %>%
      pull(count)})) %>%
  bind_rows(., tibble(var = "profiles_with_county_coords", count = {kssl.sitedata %>%
      group_by(lims.pedon.id) %>%
      summarise(lat.ycntr = mean(lat.ycntr)) %>%
      filter(!is.na(lat.ycntr)) %>%
      ungroup() %>%
      summarise(count = n()) %>%
      pull(count)}))

sitedata.summary

# Formatting to OSSL names
kssl.sitedata <- kssl.sitedata %>%
  rename(id.layer_local_c = lay.id,
         id.project_ascii_txt = submit.proj.name,
         id.dataset.site_ascii_txt = lims.pedon.id,
         layer.upper.depth_usda_cm = lay.depth.to.top,
         layer.lower.depth_usda_cm = lay.depth.to.bottom,
         layer.texture_usda_txt = texture.description,
         pedon.taxa_usda_txt = taxonomic.classification.name,
         horizon.designation_usda_txt = horizon.designation,
         observation.date.end_iso.8601_yyyy.mm.dd = fiscal.year,
         longitude.point_wgs84_dd = longitude.std.decimal.degrees,
         latitude.point_wgs84_dd = latitude.std.decimal.degrees,
         longitude.county_wgs84_dd = long.xcntr,
         latitude.county_wgs84_dd = lat.ycntr) %>%
  mutate(id.layer_local_c = as.character(id.layer_local_c),
         id.project_ascii_txt = paste0("KSSL: ", id.project_ascii_txt)) %>%
  select(id.layer_local_c, id.project_ascii_txt, id.dataset.site_ascii_txt,
         layer.upper.depth_usda_cm, layer.lower.depth_usda_cm, layer.texture_usda_txt,
         pedon.taxa_usda_txt, horizon.designation_usda_txt, observation.date.end_iso.8601_yyyy.mm.dd,
         longitude.point_wgs84_dd, latitude.point_wgs84_dd,
         longitude.county_wgs84_dd, latitude.county_wgs84_dd) %>%
  mutate(observation.date.end_iso.8601_yyyy.mm.dd = ymd(paste0(observation.date.end_iso.8601_yyyy.mm.dd, "-12-31"))) %>%
  mutate(observation.date.begin_iso.8601_yyyy.mm.dd = floor_date(observation.date.end_iso.8601_yyyy.mm.dd, "year"),
         .before = observation.date.end_iso.8601_yyyy.mm.dd) %>%
  mutate(layer.sequence_usda_uint16 = NA, .after = layer.texture_usda_txt) %>%
  mutate(location.point.error_any_m = 30,
         location.country_iso.3166_txt = "USA",
         observation.ogc.schema.title_ogc_txt = "Open Soil Spectroscopy Library",
         observation.ogc.schema_idn_url = "https://soilspectroscopy.github.io",
         surveyor.title_utf8_txt = "USDA NRCS staff",
         surveyor.contact_ietf_email = "rich.ferguson@usda.gov",
         surveyor.address_utf8_txt = "USDA-NRCS-NSSC, Federal Building, Room 152, Mail Stop, 100 Centennial Mall North, Lincoln, NE",
         dataset.title_utf8_txt = "Kellogg Soil Survey Laboratory database",
         dataset.owner_utf8_txt = "USDA, Soil and Plant Science Division, National Soil Survey Center",
         dataset.code_ascii_txt = "KSSL.SSL",
         dataset.address_idn_url = "http://ncsslabdatamart.sc.egov.usda.gov/",
         dataset.license.title_ascii_txt = "CC-BY",
         dataset.license.address_idn_url = "https://ncsslabdatamart.sc.egov.usda.gov/datause.aspx",
         dataset.doi_idf_url = "https://doi.org/10.2136/sssaj2019.06.0205",
         dataset.contact.name_utf8_txt = "Scarlett Murphy",
         dataset.contact_ietf_email = "Scarlett.Murphy@usda.gov") %>%
  mutate(id.layer_uuid_txt = openssl::md5(paste0(dataset.code_ascii_txt, id.layer_local_c)),
         id.location_olc_txt = olctools::encode_olc(latitude.point_wgs84_dd, longitude.point_wgs84_dd, 10), 
         .after = id.dataset.site_ascii_txt) %>%
  mutate_at(vars(starts_with("id.")), as.character)

# Saving version to dataset root dir
# site.rds = paste0(dirname(dir.files), "/ossl_soilsite_v1.2.rds")
# saveRDS(kssl.sitedata, site.rds)
site.qs = paste0(dirname(dir.files), "/ossl_soilsite_v1.2.qs")
qs::qsave(kssl.sitedata, site.qs, preset = "high")
```

### Soil lab information

NOTE: The code chunk below this paragraph is hidden. Just run once for getting the original names of soil properties, descriptions, data types, and units. Run once and upload to Google Sheet for formatting and integrating with the OSSL. Requires Google authentication.

<!-- ```{r soilab_overview, include=FALSE, echo=FALSE, eval=FALSE} -->
<!-- analyte <- read_csv(paste0(dir, "/All_Spectra_Access_Portable_20220712/analyte.csv")) -->
<!-- calc <- read_csv(paste0(dir, "/All_Spectra_Access_Portable_20220712/calc.csv")) -->

<!-- soillab.names <- analyte %>% -->
<!--   select(analyte.id, analyte.name, analyte.abbrev, uom.abbrev, analyte.desc) %>% -->
<!--   mutate(source = "kssl_analyte", .before = 1) %>% -->
<!--   rename(id = analyte.id, original_name = analyte.name, abbrev = analyte.abbrev, -->
<!--          unit = uom.abbrev, original_description = analyte.desc) %>% -->
<!--   bind_rows({calc %>% -->
<!--       select(calc.id, calc.name, calc.abbrev, uom.abbrev, calc.desc) %>% -->
<!--       mutate(source = "kssl_calc", .before = 1) %>% -->
<!--       rename(id = calc.id, original_name = calc.name, abbrev = calc.abbrev, -->
<!--              unit = uom.abbrev, original_description = calc.desc)}) %>% -->
<!--   arrange(original_name) %>% -->
<!--   dplyr::mutate(import = '', ossl_name = '', .after = original_name) -->

<!-- readr::write_csv(soillab.names, paste0(getwd(), "/kssl_soillab_names.csv")) -->

<!-- # Uploading to google sheet -->

<!-- # FACT CIN folder. Get ID for soildata importing table -->
<!-- googledrive::drive_ls(as_id("0AHDIWmLAj40_Uk9PVA")) -->

<!-- OSSL.soildata.importing <- "19LeILz9AEnKVK7GK0ZbK3CCr2RfeP-gSWn5VpY8ETVM" -->

<!-- # Checking metadata -->
<!-- googlesheets4::as_sheets_id(OSSL.soildata.importing) -->

<!-- # Checking readme -->
<!-- googlesheets4::read_sheet(OSSL.soildata.importing, sheet = 'readme') -->

<!-- # Preparing soillab.names -->
<!-- upload <- dplyr::as_tibble(soillab.names) -->

<!-- # Uploading -->
<!-- googlesheets4::write_sheet(upload, ss = OSSL.soildata.importing, sheet = "KSSL") -->

<!-- # Checking metadata -->
<!-- googlesheets4::as_sheets_id(OSSL.soildata.importing) -->
<!-- ``` -->

NOTE: The code chunk below this paragraph is hidden. Run once for importing the transformation rules. The table can be edited online at Google Sheets. A copy is downloaded to github for archiving.

<!-- ```{r soilab_download, include=FALSE, echo=FALSE, eval=FALSE} -->
<!-- # Downloading from google sheet -->

<!-- # FACT CIN folder id -->
<!-- listed.table <- googledrive::drive_ls(as_id("0AHDIWmLAj40_Uk9PVA"), -->
<!--                                       pattern = "OSSL_tab2_soildata_importing") -->

<!-- OSSL.soildata.importing <- listed.table[[1,"id"]] -->

<!-- # Checking metadata -->
<!-- googlesheets4::as_sheets_id(OSSL.soildata.importing) -->

<!-- # Preparing soillab.names -->
<!-- transvalues <- googlesheets4::read_sheet(OSSL.soildata.importing, sheet = "KSSL") %>% -->
<!--   filter(import == TRUE) %>% -->
<!--   select(contains(c("source", "id", "original_name", "ossl_"))) -->

<!-- # Saving to folder -->
<!-- write_csv(transvalues, paste0(getwd(), "/OSSL_transvalues.csv")) -->
<!-- ``` -->

Reading KSSL-to-OSSL transformation values:
```{r soilab_transvalues, include=TRUE, echo=TRUE, eval=TRUE}
transvalues <- read_csv(paste0(getwd(), "/OSSL_transvalues.csv"))
knitr::kable(transvalues)
```

Preparing soil data from determined and estimated analytes:
```{r soilab_preparation, include=TRUE, echo=TRUE, eval=TRUE}
dir.db <- paste0(dir.files, "/All_Spectra_Access_Portable_20220712")

# Reading tables with site info
layer <- fread(paste0(dir.db, "/layer.csv"))
sample <- fread(paste0(dir.db, "/sample.csv"))
layer.analyte <- fread(paste0(dir.db, "/layer_analyte.csv"))
result <- fread(paste0(dir.db, "/result.csv"))

# Analyte selection
analyte.sel.ids <- transvalues %>%
  filter(source == "kssl_analyte") %>%
  pull(id)

analyte.sel.names <- transvalues %>%
  filter(source == "kssl_analyte") %>%
  select(id, ossl_name, ossl_convert)

layer.analyte.sel <- layer.analyte %>%
  filter(analyte.id %in% analyte.sel.ids) %>%
  select(analyte.id, lay.id, calc.value, master.prep.id) %>%
  left_join(analyte.sel.names, by = c("analyte.id" = "id")) %>%
  rename(value = calc.value, prep = master.prep.id) %>%
  select(contains("ossl"), lay.id, value, prep)

# Calc selection
calc.sel.ids <- transvalues %>%
  filter(source == "kssl_calc") %>%
  pull(id)

calc.sel.names <- transvalues %>%
  filter(source == "kssl_calc") %>%
  select(id, ossl_name, ossl_convert)

layer.calc.sel <- result %>%
  filter(calc.id %in% calc.sel.ids) %>%
  select(calc.id, result.source.id, calc.value) %>%
  left_join(calc.sel.names, by = c("calc.id" = "id")) %>%
  rename(value = calc.value, lay.id = result.source.id) %>%
  select(contains("ossl"), lay.id, value)

# Binding determined and calculated analytes
kssl.soildata <- bind_rows(layer.analyte.sel, layer.calc.sel)

# Averaging minor repeats by unique numeric variable.
# The same soil property can have some different preps
# Only 1 NA is produced and filtered out
kssl.soildata.numeric <- kssl.soildata %>%
  filter(ossl_name != "efferv_usda.a479_class") %>%
  group_by(ossl_name, lay.id) %>%
  summarise(value = mean(as.numeric(value), na.rm = TRUE), .groups = "drop") %>%
  filter(!is.na(value))

# Passing the transforming function
# mutate(x = value) %>%
#   mutate(value = eval(parse(text = ossl_convert))) %>%
  
kssl.soildata.numeric.bind <- kssl.soildata.numeric %>%
  mutate(value = round(value, 5)) %>%
  pivot_wider(names_from = "ossl_name", values_from = value) %>%
  as.data.frame()

# The first column lay.id is preserved the same
functions.list <- transvalues %>%
  filter(ossl_name %in% names(kssl.soildata.numeric.bind)) %>%
  mutate(ossl_name = factor(ossl_name, levels = names(kssl.soildata.numeric.bind))) %>%
  arrange(ossl_name) %>%
  pull(ossl_convert) %>%
  c("x", .)

# Applying transformation rules
kssl.soildata.numeric.bind <- transform_values(df = kssl.soildata.numeric.bind,
                                               out.name = names(kssl.soildata.numeric.bind),
                                               in.name = names(kssl.soildata.numeric.bind),
                                               fun.lst = functions.list)

# Firsting minor repeats by unique character variable, i.e. only effervescence
# Checking effervescence levels
effervescence.levels <- kssl.soildata %>%
  filter(ossl_name == "efferv_usda.a479_class") %>%
  distinct(value) %>%
  pull(value)

# Simplifying levels
new.effervescence.levels <- c("none", "strong", "very slight", "violent", "slight",
                              "none", "none", "slight", "strong", "very slight",
                              "slight", "very slight", "very slight", "violent", "violent",
                              "strong", "very slight", "strong", "slight", "slight",
                              "slight", "slight", "slight", "strong", "strong",
                              "violent", "violent")

# Recoding
names(new.effervescence.levels) <- effervescence.levels

kssl.soildata.class <- kssl.soildata %>%
  filter(ossl_name == "efferv_usda.a479_class") %>%
  mutate(x = value) %>%
  mutate(value = eval(parse(text = ossl_convert))) %>%
  group_by(ossl_name, lay.id) %>%
  summarise(value = first(value), .groups = "drop") %>%
  mutate(value = recode(value, !!!new.effervescence.levels))

kssl.soildata.class %>%
  distinct(value) %>%
  pull(value)

kssl.soildata.class.bind <- kssl.soildata.class %>%
  pivot_wider(names_from = "ossl_name", values_from = value)

# Final soillab data
kssl.soildata <- left_join(kssl.soildata.numeric.bind, kssl.soildata.class.bind, by = "lay.id") %>%
  rename(id.layer_local_c = lay.id) %>%
  mutate_at(vars(starts_with("id.")), as.character)

# Checking total number of observations
kssl.soildata %>%
  distinct(id.layer_local_c) %>%
  summarise(count = n())
  
# Saving version to dataset root dir
# soillab.rds = paste0(dirname(dir.files), "/ossl_soillab_v1.2.rds")
# saveRDS(kssl.soildata, soillab.rds)
soillab.qs = paste0(dirname(dir.files), "/ossl_soillab_v1.2.qs")
qs::qsave(kssl.soildata, soillab.qs, preset = "high")
```

### Mid-infrared spectroscopy data

Reading the MIR scans. The same sample/layer can have multiple preparation. We will focus only on XS and XN prep scans.
```{r mir, include=TRUE, echo=TRUE, eval=TRUE}
dir.mir <- paste0(dir.files, "/KSSL_MIR_export")

# Already formatted to 600-4000 cm-1, with 2 cm-1 interval
mir.scans <- fread(paste0(dir.mir, "/KSSL_202207_MIR_spectra_all_avg.csv"), header = TRUE)

sample <- fread(paste0(dir.db, "/sample.csv"))

# Getting only XS and XN prep scans
imported.mir.ids <- mir.scans %>%
  rename(scan_id = sample_id) %>% # recoded to scan id = sample+prep
  select(scan_id) %>%
  as_tibble() %>%
  mutate(prep = str_sub(scan_id, -2, -1),
         sample_id = as.numeric(str_sub(scan_id, 1, -3)))

mir.selected.smp.ids <- imported.mir.ids %>%
  filter(prep %in% c("XS", "XN")) %>% 
  distinct(sample_id) %>%
  pull()

mir.selected.scan.ids <- imported.mir.ids %>%
  filter(prep %in% c("XS", "XN")) %>% 
  distinct(scan_id) %>%
  pull()

mir.scans <- mir.scans %>%
  filter(sample_id %in% mir.selected.scan.ids)

# Recoding smp.id (sample_id) to lay.id for filtering joining with other tables
mir.selected.lay.ids <- sample %>%
  filter(smp.id %in% mir.selected.smp.ids) %>%
  select(smp.id, lay.id) %>%
  rename(id.layer_local_c = lay.id, sample_id = smp.id)

# Final preparation
kssl.mir <- imported.mir.ids %>%
  filter(sample_id %in% mir.selected.smp.ids) %>%
  left_join(mir.selected.lay.ids, by = "sample_id") %>%
  relocate(id.layer_local_c, .before = 1) %>%
  rename(id.sample_local_c = sample_id, id.scan_local_c = scan_id) %>%
  select(id.scan_local_c, id.layer_local_c) %>%
  right_join(mir.scans, by = c("id.scan_local_c" = "sample_id")) %>%
  mutate_at(vars(-contains("id")), as.numeric)

old.wavenumbers <- seq(600, 4000, by = 2)
new.wavenumbers <- paste0("scan_mir.", old.wavenumbers, "_abs")

kssl.mir <- kssl.mir %>%
  rename_with(~new.wavenumbers, as.character(old.wavenumbers))

# Spectral consistency analysis

# Gaps
scans.na.gaps <- kssl.mir %>%
  select(-id.scan_local_c, -id.layer_local_c) %>%
  apply(., 1, function(x) round(100*(sum(is.na(x)))/(length(x)), 2)) %>%
  tibble(proportion_NA = .) %>%
  bind_cols({kssl.mir %>% select(id.scan_local_c)}, .)

# Extreme negative - irreversible erratic patterns
scans.extreme.neg <- kssl.mir %>%
  select(-id.scan_local_c, -id.layer_local_c) %>%
  apply(., 1, function(x) {round(100*(sum(x < -1, na.rm=TRUE))/(length(x)), 2)}) %>%
  tibble(proportion_lower0 = .) %>%
  bind_cols({kssl.mir %>% select(id.scan_local_c)}, .)

# Extreme positive, irreversible erratic patterns
scans.extreme.pos <- kssl.mir %>%
  select(-id.scan_local_c, -id.layer_local_c) %>%
  apply(., 1, function(x) {round(100*(sum(x > 5, na.rm=TRUE))/(length(x)), 2)}) %>%
  tibble(proportion_higherAbs5 = .) %>%
  bind_cols({kssl.mir %>% select(id.scan_local_c)}, .)

# Consistency summary - problematic scans
scans.summary <- scans.na.gaps %>%
  left_join(scans.extreme.neg, by = "id.scan_local_c") %>%
  left_join(scans.extreme.pos, by = "id.scan_local_c")

scans.summary %>%
  select(-id.scan_local_c) %>%
  pivot_longer(everything(), names_to = "check", values_to = "value") %>%
  filter(value > 0) %>%
  group_by(check) %>%
  summarise(count = n())

# These few scans with extreme values are removed - getting ids
extreme.ids <- scans.summary %>%
  filter(proportion_higherAbs5 > 0 | proportion_lower0 > 0) %>%
  pull(id.scan_local_c)

# Removing extremes
kssl.mir <- kssl.mir %>%
  filter(!(id.scan_local_c %in% extreme.ids))

# Metadata
metadata1 <- fread(paste0(dir.mir, "/KSSL_202207_MIR_metadata_20221003.csv"), header = TRUE) %>%
  select(sample_id, date_time_sm) %>%
  mutate(id.scan_local_c = str_sub(sample_id, 1, -3)) %>%
  mutate(scan.mir.date.begin_iso.8601_yyyy.mm.dd = ymd_hms(date_time_sm)) %>%
  mutate(scan.mir.date.begin_iso.8601_yyyy.mm.dd = date(scan.mir.date.begin_iso.8601_yyyy.mm.dd)) %>%
  select(id.scan_local_c, scan.mir.date.begin_iso.8601_yyyy.mm.dd)

metadata2 <- fread(paste0(dir.mir, "/KSSL_202207_MIR_metadata_badfiles_20221110.csv"), header = TRUE) %>%
  select(FileName, SMP_Date) %>%
  mutate(id.scan_local_c = str_sub(FileName, 1, -5)) %>%
  mutate(scan.mir.date.begin_iso.8601_yyyy.mm.dd = ymd(mdy(SMP_Date))) %>%
  select(id.scan_local_c, scan.mir.date.begin_iso.8601_yyyy.mm.dd)

metadata <- bind_rows(metadata1, metadata2)

kssl.mir.metadata <- metadata %>%
  mutate(scan.mir.date.end_iso.8601_yyyy.mm.dd = scan.mir.date.begin_iso.8601_yyyy.mm.dd,
         scan.mir.model.name_utf8_txt = "Bruker Vertex 70 with HTS-XT accessory",
         scan.mir.model.code_any_txt = "Bruker_Vertex_70.HTS.XT",
         scan.mir.method.optics_any_txt = "KBr beamsplitter; MCT detector; Roughened Aluminium background",
         scan.mir.method.preparation_any_txt = paste0(str_sub(id.scan_local_c, -2, -1), ": finely ground <80 mesh"),
         scan.mir.license.title_ascii_txt = "CC-BY",
         scan.mir.license.address_idn_url = "https://ncsslabdatamart.sc.egov.usda.gov/datause.aspx",
         scan.mir.doi_idf_url = "https://doi.org/10.2136/sssaj2019.06.0205",
         scan.mir.contact.name_utf8_txt = "Scarlett Murphy",
         scan.mir.contact.email_ietf_txt = "Scarlett.Murphy@usda.gov") %>%
  group_by(id.scan_local_c) %>%
  summarise_all(first)

# Final table
kssl.mir.export <- right_join(kssl.mir.metadata, kssl.mir, by = "id.scan_local_c") %>%
  mutate_at(vars(starts_with("id.")), as.character)

# Saving version to dataset root dir
# soilmir.rds = paste0(dirname(dir.files), "/ossl_mir_v1.2.rds")
# saveRDS(kssl.mir.export, soilmir.rds)
soilmir.qs = paste0(dirname(dir.files), "/ossl_mir_v1.2.qs")
qs::qsave(kssl.mir.export, soilmir.qs, preset = "high")
```

### Visible and Near-infrared spectroscopy data

Reading the ViSNIR scans. This comes from the RaCA project and, apparently, no VisNIR scan was added since then.
```{r visnir, include=TRUE, echo=TRUE, eval=TRUE}
dir.visnir <- paste0(dir.files, "/RACA_ViSNIR_Jun2022")

# Already formatted to 350-2500 nm, with 1 nm interval
visnir.scans <- fread(paste0(dir.visnir, "/RaCA_measured.csv"), header = TRUE)

sample <- fread(paste0(dir.db, "/sample.csv"))

# Getting smp.id. There is no duplicate
visnir.selected.smp.ids <- visnir.scans %>%
  select(smp_id) %>%
  distinct(smp_id) %>%
  pull()

# Recoding smp.id to lay.id
imported.visnir.lay.ids <- sample %>%
  filter(smp.id %in% visnir.selected.smp.ids) %>%
  select(smp.id, lay.id) %>%
  rename(id.layer_local_c = lay.id, id.sample_local_c = smp.id)

# Final preparation
kssl.visnir <- imported.visnir.lay.ids %>%
  left_join(visnir.scans, by = c("id.sample_local_c" = "smp_id")) %>%
  select(id.layer_local_c, asdfilename, starts_with("X")) %>%
  rename(id.scan_local_c = asdfilename) %>%
  mutate(id.scan_local_c = gsub("\\.asd", "", id.scan_local_c)) %>%
  mutate_at(vars(-contains("id")), as.numeric)

old.wavelengths <- paste0("X", seq(350, 2500, by = 2))
new.wavelengths <- paste0("scan_visnir.", gsub("X", "", old.wavelengths), "_ref")

kssl.visnir <- kssl.visnir %>%
  select(id.layer_local_c, id.scan_local_c, all_of(old.wavelengths)) %>%
  rename_with(~new.wavelengths, as.character(old.wavelengths))

# Spectral consistency analysis
cl = makeCluster(mc <- getOption("cl.cores", data.table::getDTthreads()))

# Gaps
scans.na.gaps <- kssl.visnir %>%
  select(-id.scan_local_c, -id.layer_local_c) %>%
  parallel::parRapply(cl, ., function(x) round(100*(sum(is.na(x)))/(length(x)), 2)) %>%
  tibble(proportion_NA = .) %>%
  bind_cols({kssl.visnir %>% select(id.scan_local_c)}, .)

# Extreme negative
scans.extreme.neg <- kssl.visnir %>%
  select(-id.scan_local_c, -id.layer_local_c) %>%
  parallel::parRapply(cl, ., function(x) {
    round(100*(sum(x < 0, na.rm=TRUE))/(length(x)), 2)
  }) %>%
  tibble(proportion_lower0 = .) %>%
  bind_cols({kssl.visnir %>% select(id.scan_local_c)}, .)

# Extreme positive
scans.extreme.pos <- kssl.visnir %>%
  select(-id.scan_local_c, -id.layer_local_c) %>%
  parallel::parRapply(cl, ., function(x) {
    round(100*(sum(x > 1, na.rm=TRUE))/(length(x)), 2)
  }) %>%
  tibble(proportion_higherRef1 = .) %>%
  bind_cols({kssl.visnir %>% select(id.scan_local_c)}, .)

stopCluster(cl)

# Consistency summary
scans.summary <- scans.na.gaps %>%
  left_join(scans.extreme.neg, by = "id.scan_local_c") %>%
  left_join(scans.extreme.pos, by = "id.scan_local_c")

scans.summary %>%
  select(-id.scan_local_c) %>%
  pivot_longer(everything(), names_to = "check", values_to = "value") %>%
  filter(value > 0) %>%
  group_by(check) %>%
  summarise(count = n())

# Metadata
kssl.visnir.metadata <- visnir.scans %>%
  select(asdfilename, scan_date) %>%
  rename(id.scan_local_c = asdfilename) %>%
  mutate(id.scan_local_c = gsub("\\.asd", "", id.scan_local_c)) %>%
  rename(scan.visnir.date.begin_iso.8601_yyyy.mm.dd = scan_date) %>%
  mutate(scan.visnir.date.begin_iso.8601_yyyy.mm.dd = ymd(mdy(scan.visnir.date.begin_iso.8601_yyyy.mm.dd)),
         scan.visnir.date.end_iso.8601_yyyy.mm.dd = scan.visnir.date.begin_iso.8601_yyyy.mm.dd) %>%
  mutate(scan.visnir.model.name_utf8_txt = "ASD Labspec 2500",
         scan.visnir.model.code_any_txt = "ASD_Labspec_2500",
         scan.visnir.method.optics_any_txt = "ASD MugLite",
         scan.visnir.method.preparation_any_txt = "Sieved <2 mm",
         scan.visnir.license.title_ascii_txt = "CC-BY",
         scan.visnir.license.address_idn_url = "https://www.nrcs.usda.gov/resources/data-and-reports/rapid-carbon-assessment-raca",
         scan.visnir.doi_idf_url = "https://doi.org/10.1007/978-3-319-04084-4_10",
         scan.visnir.contact.name_utf8_txt = "Scarlett Murphy",
         scan.visnir.contact.email_ietf_txt = "Scarlett.Murphy@usda.gov")

# Final table
kssl.visnir.export <- right_join(kssl.visnir.metadata, kssl.visnir, by = "id.scan_local_c") %>%
  mutate_at(vars(starts_with("id.")), as.character)

# Saving version to dataset root dir
# soilvisnir.rds = paste0(dirname(dir.files), "/ossl_visnir_v1.2.rds")
# saveRDS(kssl.visnir.export, soilvisnir.rds)
soilvisnir.qs = paste0(dirname(dir.files), "/ossl_visnir_v1.2.qs")
qs::qsave(kssl.visnir.export, soilvisnir.qs)
```

### Quality control

For joining site, soil lab and spectral data, we must keep in mind that soil lab, site and VisNIR do not have repeats in the `id.layer_local_c`. MIR scans, on the other hand, have repeats due to different preparation methods. MIR `id.layer_local_c` has repeats, while MIR `id.scan_local_c` doesn't. The final table must be joined as:

- MIR is used as first reference due to repeats.  
- It is fully joined with VisNIR because some MIR samples does not have VisNIR scans.  
- The result is left joined with the site and soil lab data. This drop data without any scan.

The availabilty of data is summarised below:
```{r bind_test, include=TRUE, echo=TRUE, eval=TRUE}
# Taking a few representative columns for checking the consistency of joins
kssl.availability <- kssl.mir.export %>%
  select(id.scan_local_c, id.layer_local_c, scan_mir.600_abs) %>%
  full_join({kssl.visnir.export %>%
      select(id.scan_local_c, id.layer_local_c, scan_visnir.350_ref)}, by = "id.layer_local_c") %>%
  mutate(id.scan_local_c = coalesce(id.scan_local_c.x, id.scan_local_c.y, NA), .before = 1) %>%
  select(-id.scan_local_c.x, -id.scan_local_c.y) %>%
  left_join({kssl.sitedata %>%
      select(id.layer_local_c, layer.upper.depth_usda_cm)}, by = "id.layer_local_c") %>%
  left_join({kssl.soildata %>%
      select(id.layer_local_c, c.tot_usda.a622_w.pct)}, by = "id.layer_local_c") %>%
  filter(!is.na(id.layer_local_c))

# Availability of information from KSSL
kssl.availability %>%
  mutate_all(as.character) %>%
  pivot_longer(everything(), names_to = "column", values_to = "value") %>%
  filter(!is.na(value)) %>%
  group_by(column) %>%
  summarise(count = n())

# Repeats check - There are 60 samples duplicated due to different MIR preparations
kssl.availability %>%
  mutate_all(as.character) %>%
  select(id.scan_local_c, id.layer_local_c) %>%
  pivot_longer(everything(), names_to = "column", values_to = "value") %>%
  group_by(column, value) %>%
  summarise(repeats = n()) %>%
  group_by(column, repeats) %>%
  summarise(count = n())
```

This summary shows that, at total, about 92k observations are available. Some rows have both MIR and VisNIR scans, many not. As we have repeats for MIR (due to different preparations), it means that for the same layer id we can have different MIR scans, but similar VisNIR scans as they are repeated when joining.

NOTE: As the duplicated layers represent a minor fraction (n=60) of the database, they will be dropped when binding  all the datasets for making OSSL level 0.

Plotting sites map:
```{r map, include=TRUE, echo=TRUE, eval=TRUE}
data("World")

points <- kssl.sitedata %>%
  filter(!is.na(longitude.point_wgs84_dd)) %>%
  st_as_sf(coords = c('longitude.point_wgs84_dd', 'latitude.point_wgs84_dd'), crs = 4326)

tmap_mode("plot")

tm_shape(World) +
  tm_polygons('#f0f0f0f0', border.alpha = 0.2) +
  tm_shape(points) +
  tm_dots()
```

Soil analytical data summary:
```{r summary, include=TRUE, echo=TRUE, eval=TRUE}
kssl.soildata %>%
  mutate(id.layer_local_c = factor(id.layer_local_c)) %>%
  skimr::skim() %>%
  dplyr::select(-numeric.hist, -complete_rate)
```

MIR spectral visualization:
```{r mir_plot, include=TRUE, echo=TRUE, eval=TRUE}
set.seed(1993)
kssl.mir %>%
  sample_n(1000) %>%
  tidyr::pivot_longer(-all_of(c("id.scan_local_c", "id.layer_local_c")),
                      names_to = "wavenumber", values_to = "absorbance") %>%
  dplyr::mutate(wavenumber = gsub("scan_mir.|_abs", "", wavenumber)) %>%
  dplyr::mutate(wavenumber = as.numeric(wavenumber)) %>%
  ggplot(aes(x = wavenumber, y = absorbance, group = id.scan_local_c)) +
  geom_line(alpha = 0.1) +
  scale_x_continuous(breaks = c(600, 1200, 1800, 2400, 3000, 3600, 4000)) +
  labs(x = bquote("Wavenumber"~(cm^-1)), y = "Absorbance") +
  theme_light()
```

ViSNIR spectral visualization:
```{r visnir_plot, include=TRUE, echo=TRUE, eval=TRUE}
set.seed(1993)
kssl.visnir %>%
  sample_n(1000) %>%
  mutate_at(vars(starts_with("scan_visnir.")), as.numeric) %>%
  tidyr::pivot_longer(-all_of(c("id.scan_local_c", "id.layer_local_c")), names_to = "wavelength", values_to = "reflectance") %>%
  dplyr::mutate(wavelength = gsub("scan_visnir.|_ref", "", wavelength)) %>%
  dplyr::mutate(wavelength = as.numeric(wavelength)) %>%
  ggplot(aes(x = wavelength, y = reflectance, group = id.scan_local_c)) +
  geom_line(alpha = 0.1) +
  scale_x_continuous(breaks = c(350, 500, 1000, 1500, 2000, 2500)) +
  labs(x = bquote("Wavelength"~(nm)), y = "Reflectance") +
  theme_light()
```

```{r}
toc()
rm(list = ls())
gc()
```

## References
